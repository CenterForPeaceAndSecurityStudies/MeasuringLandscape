---
title: "Train Supervised Fuzzy Toponym Matcher"
author: "Rex W. Douglass"
date: "9/12/2016"
output:
  html_notebook:
    number_sections: yes
    theme: readable
    toc: yes
editor_options: 
  chunk_output_type: inline
---

There is where we train our fuzzy matcher

```{r }
library(MeasuringLandscapeCivilWar)
global_loads()

knitr::opts_knit$set(progress = TRUE, verbose = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8,  warning=FALSE, message=FALSE, cache=TRUE)
options(width = 160)

```

It is based on an events dataset built and cleaned in another file.

```{r}

#Load Events
events_sf <- readRDS(system.file("extdata", "MeasuringLandscapeCivilWar_events_cleaned.Rdata", package = "MeasuringLandscapeCivilWar")) %>% distinct() 
events_dt <- as.data.table(events)
dim(events)

sort(unique(unlist(strsplit(events_sf$name_cleaner,"")))) #ok only number and lowercase letters from now on

#Avoid creating geometries where one of the two is NA
events_sf$map_coordinate_clean_longitude[is.na(events_sf$map_coordinate_clean_latitude)] <- NA
events_sf$map_coordinate_clean_latitude[is.na(events_sf$map_coordinate_clean_longitude)] <- NA

events_sf <- events_sf  %>% st_as_sf(coords = c("map_coordinate_clean_longitude", "map_coordinate_clean_latitude"), crs = 4326,agr = "constant", remove=F, na.fail =F)

valid <- st_is_valid(events_sf$geometry); table(valid)


events_sf_text_coord_unique <- ddply(events_sf[,c('location_text','name_clean','name_cleaner','document_district_clean','map_coordinate_clean_latitude','map_coordinate_clean_longitude')],
                                     .(location_text), transform,
      map_coordinate_has =sum(!is.na(map_coordinate_clean_latitude))
      )

#Load Gazeteers
flatfiles_sf_roi <- readRDS(system.file("extdata", "flatfiles_sf_roi.Rdata", package = "MeasuringLandscapeCivilWar")) 
dim(flatfiles_sf_roi)
flatfiles_dt <- as.data.table(flatfiles_sf_roi)
setkey(flatfiles_dt,place_hash)


```

Summary statistics

```{r}

table( !is.na( st_coordinates(events_sf)[,1] ) ,
       !is.na(events_sf$name_cleaner) )

```


# Produce a dataset for hand labeling

This takes every event with a coordinate, and looks up the 10 nearest points in the gazeteers. It saves this as a csv file that a human can label as a toponym match or mismatch.

```{r}
#Create some UTM versions because we want to be precise about distances in meters

crs_m <- "+proj=utm +zone=27 +datum=NAD83 +units=m +no_defs" 
flatfiles_sf_roi_utm_roi_centroid <-  st_centroid(
                                              st_transform( flatfiles_sf_roi, crs=crs_m)
                                  ) ; dim(flatfiles_sf_roi_utm_roi_centroid)

events_sf_utm <-  st_transform(
                              events_sf,
                           crs=crs_m) ; dim(events_sf_utm)

#We're not doing this anymore we're sampling
#This is going to be our main pairwise dataset, I think we'll add columns to it as necessary rather than just try to rbind the two main ones over and over again
#events_flatfiles <- as.data.table( expand.grid(event_hash = events_sf_utm$event_hash,
#                                               place_hash = flatfiles_sf_roi_utm_roi_centroid$place_hash) ) #this takes a while, might want to mcapply over a
#
#
#dim(events_flatfiles) #549,004,829 it's half a billion observations
#

#You know, centroids should be enough. If the river or polygon is much bigger than this then I shouldn't be merging it anyway
p_load(RANN)
coords_events <- st_coordinates(events_sf_utm)
coords_events[!is.finite(coords_events)] <- NA
condition_events <- !is.na(coords_events[,1]); table(condition_events)

coords_flatfiles <- st_coordinates(flatfiles_sf_roi_utm_roi_centroid)
coords_flatfiles[!is.finite(coords_flatfiles)] <- NA
condition_flatfiles <- !is.na(coords_flatfiles[,1]); table(condition_flatfiles)

nearest_gaz <- nn2(
                 data  = na.omit(coords_flatfiles),
                 query = na.omit(coords_events) ,
                 k = 10, #wow this actually ended up mattering
                 #treetype = c("kd", "bd"),
                 searchtype = "standard" #,
                 #radius = 4000 #meters
               ) #damn that's fast

#table(nearest$nn.dists>100) #These are missing, no match within the radius
#summary(nearest_gaz$nn.dists[nearest_gaz$nn.dists<100])
nearest_long <- as.data.table(nearest_gaz$nn.idx)
nearest_long$event_hash <- events_sf_utm$event_hash[condition_events]
library(reshape2)
m_within <- melt(nearest_long, id.vars=c("event_hash"))
#m_within$variable <- NULL
m_within$place_hash <- flatfiles_sf_roi_utm_roi_centroid$place_hash[condition_flatfiles][m_within$value]
m_within$value <- NULL

setkey(flatfiles_dt, place_hash)

events_dt <- as.data.table(events_sf)
setkey(events_dt, event_hash)

m_within$name_cleaner_a <-  events_dt[m_within$event_hash, ]$name_cleaner
m_within$name_cleaner_b <-  flatfiles_dt[m_within$place_hash, ]$name_cleaner

setkey(m_within, event_hash, variable)

m_within$rex_match <- NA
m_within$rex_match[m_within$name_cleaner_a==m_within$name_cleaner_b] <- 1

m_within <- subset(m_within, !duplicated(paste(name_cleaner_a,name_cleaner_b)))
m_within$string_dist_osa <-  stringdist(m_within$name_cleaner_a, m_within$name_cleaner_b, method ="osa", nthread=48)

m_within <- subset(m_within, !is.na(name_cleaner_a) & !is.na(name_cleaner_b) )
 
write.csv(m_within, 
          "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/event_flatfile_matches_for_hand_labeling.csv")

```

# Creat Training and Test Data

This loads a hand labeled training file and optionally adds extra negative examples.

```{r}

create_training_dataset <- function(vars_id, vars_weights, vars_y, vars_x, neg_count=0, fromscratch=F, drop_zero_dist=T, drop_identical=T) {
  
  if(fromscratch | neg_count>0){
      handlabeled <- fread(
        '/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar_TooBig/event_flatfile_matches_for_hand_labeling - event_flatfile_matches_for_hand_labeling.csv',
                           data.table=T) 
      #handlabeled <- fread("/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/name_cleaner_pairs.csv")

      handlabeled$extranegative <- F
      handlabeled <- subset(handlabeled, name_cleaner_a!="" & name_cleaner_b!="")
      handlabeled$exact_match <- handlabeled$name_cleaner_a== handlabeled$name_cleaner_b
      dim(handlabeled)
      
      temp <- handlabeled %>% group_by(name_cleaner_a) %>% summarize_at('rex_match', sum, na.rm = TRUE)
      dim(temp)
      
      #Remove the identicals. That's giving it a false confidence
      handlabeled <- subset(handlabeled, name_cleaner_a != name_cleaner_b)
      table(handlabeled$rex_match)
      
      #Stopped including examples outside the ROI
      # I think our move here now is to supplement it with lots and lots of randomly sampled zeros that are sure to be more than 10km away
      # outside_roi <- subset(flatfiles_dt, !region_of_interest_within)$place_hash #one of those things that should take no time but is getting hung up for some reason
      # #Intentionally throws a warning
      # neg_count <- neg_count #how many artificial negative examples to include
      # neg1 <- data.table( event_hash=sample(events_sf_utm$event_hash[!is.na(events_sf_utm$name_cleaner)], size=neg_count, replace = T),
      #                     place_hash=sample(outside_roi, size=neg_count, replace = T) ) #intentionally letting it recycle to match places length
      # setkey(events_dt, event_hash)
      # neg1$name_cleaner_a <-  events_dt[neg1$event_hash, ]$name_cleaner
      # neg1$name_cleaner_b <-  flatfiles_dt[neg1$place_hash, ]$name_cleaner
      # neg1$rex_match <- 0
      # dim(neg1)
      # neg1$extranegative <- T
      # 
      # handlabeled <- rbindlist(list(handlabeled, neg1), fill=T) ; dim(handlabeled) #combine the two
      
      if(drop_identical){
        handlabeled <- handlabeled[name_cleaner_a != name_cleaner_b] ; dim(handlabeled)
      }
      if(drop_zero_dist){
        
        handlabeled[,temp_q_cos:= stringsim(name_cleaner_a,name_cleaner_b,"cos", nthread=48,q=2),]
        handlabeled <- handlabeled[temp_q_cos>.3] ; dim(handlabeled)

        #handlabeled[,temp_q_gram_2:= stringsim(name_cleaner_a,name_cleaner_b,"qgram", nthread=48,q=2),]
        #handlabeled <- handlabeled[temp_q_gram_2 != 0] ; dim(handlabeled)
      }
            
      #Need the model to be indifferent to ordering so add the training obs back in flipped as well
      handlabeled2 <- handlabeled
      handlabeled2$temp <- handlabeled2$name_cleaner_a
      handlabeled2$name_cleaner_a <- handlabeled2$name_cleaner_b
      handlabeled2$name_cleaner_b <- handlabeled2$temp
      handlabeled2$temp <- NULL
      handlabeled <- rbind(handlabeled, handlabeled2) ; dim(handlabeled)
      handlabeled <- subset(handlabeled, !duplicated(paste(name_cleaner_a,name_cleaner_b)))
      
      #Do it again after the rbind
      handlabeled <- subset(handlabeled, name_cleaner_a!="" & name_cleaner_b!="")
      dim(handlabeled)
      #Remove the identicals. That's giving it a false confidence
      

      
      table(handlabeled$rex_match) #2,123 positive examples, but half of those are just the mirror
      

      handlabeled$a <- handlabeled$name_cleaner_a
      handlabeled$b <- handlabeled$name_cleaner_b
      
      postfixes=geonames_postfixes()
      print("Stemming A")
      stem_results_a <- strip_postfixes(to_be_striped=handlabeled$a , postfixes=postfixes, whitelist="fort hall", verbose=T) 
      print("Stemming B")
      stem_results_b <- strip_postfixes(to_be_striped=handlabeled$b , postfixes=postfixes, whitelist="fort hall", verbose=T) 
      stem_ab <- data.table(a=stem_results_a$name_cleaner_stemmed,b=stem_results_b$name_cleaner_stemmed)
      
      handlabeled <- toponym_add_features(handlabeled) #requires the two columns to be called a and b appends all the columns necessary for toponym matching
      temp <- toponym_add_features(stem_ab) #requires the two columns to be called a and b appends all the columns necessary for toponym matching
      names(temp) <- paste0(names(temp),"_stemmed")
      
      handlabeled <- cbind(handlabeled, temp)
      handlabeled$postfix_has_a <- stem_results_a$name_cleaner_suffix!=""
      handlabeled$postfix_has_b <- stem_results_b$name_cleaner_suffix!=""
      
      handlabeled$weights <- 1
      handlabeled$weights[handlabeled$rex_match==1] <- sum(handlabeled$rex_match==0, na.rm=T)/sum(handlabeled$rex_match==1, na.rm=T)
      table(handlabeled$weights)
    
      dim(handlabeled)
      saveRDS(handlabeled, file="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/handlabeled.Rds")
    }
    handlabeled <- readRDS(system.file("extdata", "handlabeled.Rds", package = "MeasuringLandscapeCivilWar"))

    vars_id_y_x_weights <- c(vars_id,vars_y,vars_x,vars_weights)
    
    #id_test <- sample(unique(handlabeled$a_stemmed),500) #sample stems instead as a harder test
    #need to save these now and never create them again. We're evaluating multiple models and if we ever do a different split we'll be cheating
    #saveRDS(id_test, file="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/id_test.Rds")
    id_test <- readRDS(system.file("extdata", "id_test.Rds", package = "MeasuringLandscapeCivilWar"))

    handlabeled$test <- F
    handlabeled$test[handlabeled$a_stemmed %in% id_test | handlabeled$b_stemmed %in% id_test] <- T
    table(handlabeled$test)
    
    #table(handlabeled[,'q_gram_2']==0, handlabeled$rex_match)
    handlabeled <- subset(handlabeled, q_gram_2>0)
    
    xy_all <- subset(handlabeled, !is.na(rex_match))[,vars_id_y_x_weights,with=F]
    xy_all[is.na(xy_all)] <- NA
    xy_train <- subset(xy_all, !test)
    xy_test <- subset(xy_all, test)
    dim(xy_test)
    dim(xy_train)
    
    return(
           list(handlabeled=handlabeled,
             xy_all=xy_all,
             xy_train=xy_train,
             xy_test=xy_test
           )
    )

}

```


# Train the model

This trains an xgboost model predincting matches a function of string distances and other features

```{r}

library(xgboost)
train_an_xb <- function(xy_train,
                        xy_test,
                        vars_x,
                        param,
                        #savefile="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/toponym_xb_model.bin",
                        use_weights=T,
                        missing=NA,
                        weight=NULL,
                        extract_features=F,
                        early_stopping_rounds=10,
                        nrounds=200
                        ){
  
  #https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-train.R
  
  #install.packages("drat", repos="https://cran.rstudio.com")
  #drat:::addRepo("dmlc")
  #install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")
  #install.packages("xgboost")
  
  label=xy_train$rex_match
  weight <- xy_train$weights # as.numeric(dtrain[[32]]) * testsize / length(label)
  
  if(use_weights){
    dtrain <- xgb.DMatrix(data=as.matrix( as.data.frame(xy_train)[,vars_x] ), label = as.numeric(as.data.frame(xy_train)$rex_match),
                          weight = xy_train$weights, missing = missing )
    dtest <- xgb.DMatrix(data= as.matrix(as.data.frame(xy_test)[,vars_x] ),
                         label = as.numeric(as.data.frame(xy_test)$rex_match),
                         weight = xy_test$weights,missing = missing )
  } else {
        dtrain <- xgb.DMatrix(data=as.matrix( as.data.frame(xy_train)[,vars_x] ), label = as.numeric(as.data.frame(xy_train)$rex_match),
                          missing = missing )
        dtest <- xgb.DMatrix(data= as.matrix(as.data.frame(xy_test)[,vars_x] ),
                             label = as.numeric(as.data.frame(xy_test)$rex_match),
                             missing = missing )
  }
  
  base_score = sum(xy_train$rex_match) / nrow(xy_train)
  watchlist <- list("dtrain" = dtrain, "dtest"=dtest)
  
  xb <- xgb.train(params=param,
                  data=dtrain,
                  nrounds = nrounds,
                  watchlist=watchlist,
                  early_stopping_rounds=early_stopping_rounds, 
                  maximize=T
                  #feval = area_under_pr_curve_metric
                  #base_score = base_score, #the initial prediction score of all instances, global bias
                  #weight=weight
                  )
  
  if(extract_features){
    new.features.train <- xgb.create.features(model = xb, as.matrix( as.data.frame(xy_train)[,vars_x]))
    new.features.test <- xgb.create.features(model = xb, as.matrix( as.data.frame(xy_test)[,vars_x]))
    
    new.dtrain <- xgb.DMatrix(data = new.features.train, label = xy_train$rex_match, missing=missing)
    new.dtest <- xgb.DMatrix(data = new.features.test, label = xy_test$rex_match, missing=missing)
    watchlist <- list(train = new.dtrain, test=new.dtest)
  
    xb2 <- xgb.train(params=param,
                  data=new.dtrain,
                  nrounds = nrounds,
                  watchlist=watchlist,
                  early_stopping_rounds=early_stopping_rounds, 
                  maximize=T
                  #feval = area_under_pr_curve_metric
                  #base_score = base_score, #the initial prediction score of all instances, global bias
                  #weight=weight
                  )
    
    xgb.save(xb2, savefile)
    
    return(xb2)
    
  } else {
    
    return(xb)
    
  }
  
}


```

```{r}

vars_id <- c("name_cleaner_a","name_cleaner_b",'test','extranegative')
vars_weights <- c("weights")
vars_y <- c("rex_match")

vars_x_string <- c(#"exact_match",               
"Jaro",
"Optimal_String_Alignment"    ,
"Levenshtein",
"Damerau_Levenshtein"    ,
"Longest_Common_Substring"     ,
"q_gram_1",
"q_gram_2",
"q_gram_3",
"q_gram_4",
"q_gram_5",
'Cosine_1',
'Cosine_2',
'Cosine_3',
'Cosine_4',
'Cosine_5',
"Jaccard"              ,
 "First_Mistmatch"         ,
"a_nchar"     ,
"b_nchar"   ,
"ab_nchar_diff"       ,             
"dJaro",
"dOptimal_String_Alignment"      ,
"dLevenshtein"     ,
"dDamerau_Levenshtein"  ,           
"dLongest_Common_Substring",
"dq_gram",
"dCosine",
"dJaccard"
# "OM",
# "OMloc",
# "OMslen"
# ,"OMspell",
# "TWED",
# "LCS",                 
# "LCP",
# "RLCP",
# "NMS",
# "NMSMST",                 
# "SVRspell",
# "CHI2"
) 

vars_x_stem <- paste0(vars_x_string, "_stemmed")

vars_x_string_corpus <- c(

"corpus_mention_count_a",
"corpus_mention_year_min_a",
"corpus_mention_year_median_a",
"corpus_mention_year_mean_a",
"corpus_mention_year_max_a",

"corpus_mention_count_b",
"corpus_mention_year_min_b",
"corpus_mention_year_median_b",
"corpus_mention_year_mean_b",
"corpus_mention_year_max_b",

"gazeteer_mentions_count_a",
"gazeteer_mentions_count_b",
"gazeteer_stem_mentions_count_a",
"gazeteer_stem_mentions_count_b"

#"postfix_has_a"
#"postfix_has_b" 

#"ngram_a", #Don't do , it's just a word count and it's missing if it's never found
#"ngram_b"
)


vars_x_stem_corpus <- paste0(vars_x_string_corpus, "_stemmed")
vars_x_everything <-  c(
                        vars_x_string,
                        vars_x_stem#,
                        #vars_x_string_corpus, #excluding corpus features
                        #vars_x_stem_corpus  #excluding corpus features
                        )     
vars_x_onlystring <- c(vars_x_string)
vars_x_stringcorpus <-  c(vars_x_string,  vars_x_string_corpus)     
vars_x_string_and_stem <- c(vars_x_string, vars_x_stem)

if(fromscratch){    
    temp_everything <- create_training_dataset(vars_id=vars_id,
                                               vars_weights=vars_weights,
                                               vars_y=vars_y,
                                               vars_x=vars_x_everything,
                                               neg_count=0, #you lose so many to cosine distance
                                               fromscratch=T,
                                               drop_zero_dist=F,
                                               drop_identical=T)
    
    saveRDS(temp_everything, file="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/temp_everything.Rds")
}
temp_everything <- readRDS(system.file("extdata", "temp_everything.Rds", package = "MeasuringLandscapeCivilWar"))
  

temp <- as.data.frame(temp_everything[[1]][,vars_x_string, with=F])
for(i in 1:ncol(temp)){
  print(colnames(temp)[i])
  print(summary(
    temp[temp_everything[[1]]$rex_match==1,i] 
    )
    )
}


```

```{r}

p_load(PRROC)
area_under_pr_curve_metric <- function(preds,dtrain){
  preds <- 1/(1 + exp(-preds))
  labels <- getinfo(dtrain, "label")
  pr <- pr.curve( preds, labels )
  return(list(metric="PRROC", value=pr$auc.integral))
}


logregobj <- function(preds, dtrain) {
  labels <- getinfo(dtrain, "label")
  preds <- 1/(1 + exp(-preds))
  grad <- preds - labels
  hess <- preds * (1 - preds)
  return(list(grad = grad, hess = hess))
} 
  
#sumwpos <- sum(weight * (label==1))
#sumwneg <- sum(weight * (label==0))

```

```{r}

require(xgboost)
require(methods)
global_test <- temp_everything[['handlabeled']] %>% filter(test %in% T  & !is.na(rex_match) )

sumwneg=sum(temp_everything[['xy_train']]$rex_match==0)
sumwpos=sum(temp_everything[['xy_train']]$rex_match==1)

# param1 <- list("objective" ="binary:logistic", #"objective" = logregobj,
#               "scale_pos_weight" = sumwneg / sumwpos,
#               "eta" = 0.3,
#               "max_depth" = 6,
#               "eval_metric" = "auc",
#               #"eval_metric" = area_under_pr_curve_metric,
#               #"eval_metric" = "ams@0.15",
#               "silent" = 1,
#               "nthread" = 48,
#               'maximize'=T)
# 
# ####Model Everything
# toponym_xb_everything <- train_an_xb(temp_everything[['xy_train']],
#                                      temp_everything[['xy_test']] ,
#                                      vars_x=vars_x_everything,
#                                      param=param1,
#                                      use_weights=F,
#                                      extract_features=F,
#                                      missing=NA )

# toponym_xb_everything_extracted <- train_an_xb(temp_everything[['xy_train']],
#                                      temp_everything[['xy_test']] ,
#                                      vars_x=vars_x_everything,
#                                      param=param1,
#                                      use_weights=F,
#                                      extract_features=T,
#                                      missing=NA,
#                   savefile="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/toponym_xb_everything_extracted.bin" )

sumwneg=sum(temp_everything[['xy_train']]$rex_match==0)
sumwpos=sum(temp_everything[['xy_train']]$rex_match==1)
param2 <- list("objective" = logregobj, #"objective" ="binary:logistic", #
              "scale_pos_weight" = sumwneg / sumwpos,
              "eta" = 0.3,
              "max_depth" = 6,
              "eval_metric" = "auc",
              #"eval_metric" = area_under_pr_curve_metric,
              #"eval_metric" = "ams@0.15",
              "silent" = 1,
              "nthread" = 48,
              'maximize'=T)

toponym_xb_everything2 <- train_an_xb(temp_everything[['xy_train']],
                                     temp_everything[['xy_test']] ,
                                     vars_x=vars_x_everything,
                                     param=param2,
                                     use_weights=F,
                                     extract_features=F )
xgb.save(toponym_xb_everything2,
         "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/toponym_xb_everything2.bin") #Have to save seperately for some reason

# sumwneg=sum(temp_everything_extraneg[['xy_train']]$rex_match==0)
# sumwpos=sum(temp_everything_extraneg[['xy_train']]$rex_match==1)
# param3 <- list("objective" = logregobj, #"objective" ="binary:logistic", #
#               "scale_pos_weight" = sumwneg / sumwpos,
#               "eta" = 0.3,
#               "max_depth" = 6,
#               "eval_metric" = "auc",
#               #"eval_metric" = area_under_pr_curve_metric,
#               #"eval_metric" = "ams@0.15",
#               "silent" = 1,
#               "nthread" = 48,
#               'maximize'=T)
# toponym_xb_everything3 <- train_an_xb(temp_everything_extraneg[['xy_train']],
#                                      temp_everything_extraneg[['xy_test']] ,
#                                      vars_x=vars_x_everything,
#                                      param=param3,
#                                      use_weights=F,
#                                      extract_features=F,
#                   savefile="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/toponym_xb_model_everything3.bin" )




# #Isn't running as multithreaded for some reason
# p_load(randomForestSRC)
# rf <- rfsrc(rex_match ~ .,  na.action = "na.impute",
#             data= cbind(rex_match= as.factor(temp_everything[['xy_train']]$rex_match), temp_everything[['xy_train']][,vars_x_everything, with=F] ),
#             case.wt=temp_everything[['xy_train']]$weights
#             )


#Subsets of variables


toponym_xb_onlystring <- train_an_xb(temp_everything[['xy_train']],
                                     temp_everything[['xy_test']] ,
                                     vars_x=vars_x_onlystring,
                                     param=param2,
                                     use_weights=F,
                                     extract_features=F,
                                     missing=NA)
xgb.save(toponym_xb_onlystring,
         "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/toponym_xb_onlystring.bin") #Have to save seperately for some reason



#toponym_xb_stringcorpus <- train_an_xb(temp_everything[['xy_train']],
#                                     temp_everything[['xy_test']] ,
#                                     vars_x=vars_x_stringcorpus,
#                                     param=param2,
#                                     use_weights=F,
#                                     extract_features=F,
#                                     missing=NA)


toponym_xb_string_and_stem <- train_an_xb(temp_everything[['xy_train']],
                                     temp_everything[['xy_test']] ,
                                     vars_x=vars_x_string_and_stem,
                                     param=param2,
                                     use_weights=F,
                                     extract_features=F,
                                     missing=NA)



```



```{r}

dtest<-xgb.DMatrix(data= as.matrix(as.data.frame(global_test)[,vars_x_everything]),
                   label=as.numeric(global_test$rex_match),
                   weight = global_test$weights,
                   missing = NA)

#dtest_extracted <- xgb.create.features(model = toponym_xb_everything, as.matrix( as.data.frame(global_test)[,vars_x]))
#new.dtest <- xgb.DMatrix(data = dtest_extracted, label = global_test$rex_match)

#global_test$toponym_xb_everything <- predict(toponym_xb_everything, dtest ) #Only the first ones uses the normal cost function, all the other ones have to bre exponentiated

#global_test$toponym_xb_everything_extracted <- predict(toponym_xb_everything_extracted, new.dtest )
global_test$toponym_xb_everything2 <- predict(toponym_xb_everything2, dtest )
global_test$toponym_xb_everything2 <- 1/(1 + exp(-global_test$toponym_xb_everything2))

global_test$toponym_xb_onlystring <- predict(toponym_xb_onlystring, dtest )
global_test$toponym_xb_onlystring <- 1/(1 + exp(-global_test$toponym_xb_onlystring))


global_test$toponym_xb_string_and_stem <- predict(toponym_xb_string_and_stem, dtest )
global_test$toponym_xb_string_and_stem <- 1/(1 + exp(-global_test$toponym_xb_string_and_stem))

#global_test$toponym_xb_stringcorpus <- predict(toponym_xb_stringcorpus, dtest )
#global_test$toponym_xb_stringcorpus <- 1/(1 + exp(-global_test$toponym_xb_stringcorpus))

# global_test$toponym_xb_everything2_extra <- predict(toponym_xb_everything2_extra, dtest )
# global_test$toponym_xb_everything2_extra <- 1/(1 + exp(-global_test$toponym_xb_everything2_extra))
# 
# global_test$toponym_xb_onlystring_extra <- predict(toponym_xb_onlystring_extra, dtest )
# global_test$toponym_xb_onlystring_extra <- 1/(1 + exp(-global_test$toponym_xb_onlystring_extra))
# 
# global_test$toponym_xb_everything2_extra_large <- predict(toponym_xb_everything2_extra_large, dtest )
# global_test$toponym_xb_everything2_extra_large <- 1/(1 + exp(-global_test$toponym_xb_everything2_extra_large))
# 
# global_test$toponym_xb_onlystring_extra_large <- predict(toponym_xb_onlystring_extra_large, dtest )
# global_test$toponym_xb_onlystring_extra_large <- 1/(1 + exp(-global_test$toponym_xb_onlystring_extra_large))
# 
# global_test$toponym_xb_everything2_uber_large <- predict(toponym_xb_everything2_uber_large, dtest )
# global_test$toponym_xb_everything2_uber_large <- 1/(1 + exp(-global_test$toponym_xb_everything2_uber_large))
# 
# global_test$toponym_xb_onlystring_uber_large <- predict(toponym_xb_onlystring_uber_large, dtest )
# global_test$toponym_xb_onlystring_uber_large <- 1/(1 + exp(-global_test$toponym_xb_onlystring_uber_large))
# 
# table(global_test$rex_match, global_test$toponym_xb_onlystring_uber_large>.5)
# 


#global_test$toponym_xb_onlystring_tiny <- predict(toponym_xb_onlystring_tiny, dtest )

#rf_predictions <-  predict(rf, newdata=global_test[,vars_x_everything] , na.action = "na.impute" )
#global_test$toponym_rf_everything <-rf_predictions$predicted[,2]




table(global_test$rex_match, global_test$toponym_xb_everything>.5)

#table(global_test$rex_match, global_test$toponym_xb_everything_extracted>.5)a#table(global_test$rex_match, global_test$toponym_rf_everything>.5)
table(global_test$rex_match, global_test$toponym_xb_everything2>.5)
#table(global_test$rex_match, global_test$toponym_xb_everything3>.5)
#table(global_test$rex_match, global_test$toponym_xb_everything4>.5)

table(global_test$rex_match, global_test$toponym_xb_onlystring>.5)

# global_test$toponym_cnn  <- as.vector(
#                          predict(cnn_all_distance,
#                                list(as.matrix(sequences_a_df_test),
#                                      as.matrix(sequences_b_df_test),
#                                      input_ab_test)
#                          , verbose=1)
# )


p_load(Metrics)
p_load(MLmetrics)
tocompare <- c("toponym_xb_everything2",
               "toponym_xb_onlystring",
               #"toponym_xb_stringcorpus",
               "toponym_xb_string_and_stem") # "toponym_xb_everything",, "toponym_xb_everything2_extra","toponym_xb_onlystring_extra","toponym_xb_everything2_extra_large", "toponym_xb_onlystring_extra_large", "toponym_xb_everything2_uber_large" , "toponym_xb_onlystring_uber_large"
print("ce")
sapply(tocompare, function(q) {  ce(actual=global_test$rex_match, predicted=global_test[,q]>.5)  })
print("F1_Score")
sapply(tocompare, function(q) {  F1_Score(global_test$rex_match, as.numeric( global_test[,q]>.5) )  })
print("logloss")
sapply(tocompare, function(q) {  logLoss(actual=global_test$rex_match, predicted=global_test[,q])  })
print("confusion")
sapply(tocompare, function(q) {  table(global_test$rex_match,  global_test[,q]>.5)  })


p_load(precrec)
msmdat1 <- evalmod( mmdata(scores=list(
                                #global_test$toponym_xb_everything,
                                #global_test$toponym_rf_everything,
                                #global_test$toponym_xb_everything_extracted,
                                global_test$toponym_xb_everything2, #highest PRC
                                #global_test$toponym_xb_everything3,
                                #global_test$toponym_xb_everything4,
                                global_test$toponym_xb_string_and_stem,
                                global_test$toponym_xb_onlystring#,
                                #global_test$toponym_xb_stringcorpus#,
                                #global_test$toponym_xb_onlystring_tiny
                                #global_test$toponym_xb_everything2_extra,
                                #global_test$toponym_xb_onlystring_extra,
                                #global_test$toponym_xb_everything2_extra_large,
                                #global_test$toponym_xb_onlystring_extra_large,
                                #global_test$toponym_xb_everything2_uber_large,
                                #global_test$toponym_xb_onlystring_uber_large
                                ),
                           labels=as.numeric(as.data.frame(global_test)$rex_match),
                           modnames = c(#"toponym_xb_everything",
                                        #"toponym_rf_everything",
                                        #"toponym_xb_everything_features",
                                        "toponym_xb_everything2",
                                        #"toponym_xb_everything3",
                                        #"toponym_xb_everything4",
                                        "toponym_xb_string_and_stem",
                                        "toponym_xb_onlystring"#,
                                        #"toponym_xb_stringcorpus"#,
                                        #"toponym_xb_onlystring_tiny"
                                        #"toponym_xb_everything2_extra",
                                        #"toponym_xb_onlystring_extra",
                                        #"toponym_xb_everything2_extra_large", 
                                        #"toponym_xb_onlystring_extra_large",
                                        #"toponym_xb_everything2_uber_large" ,
                                        #"toponym_xb_onlystring_uber_large"
                                        )
                           ) )
msmdat1
autoplot(msmdat1)


```


Evaluate the model

```{r}
p_load(DiagrammeR)
#xgb.plot.tree(model = xb)
#xgb.plot.tree(model = toponym_xb_everything2, trees = 0, show_node_id = TRUE, render = TRUE)

#importance_importance <- xgb.importance( model = xb2)
#xgb.ggplot.deepness(xb2)
importance_importance <- xgb.importance(feature_names=vars_x_everything, model = toponym_xb_everything2)
xgb.plot.importance(importance_importance)

importance_importance <- xgb.importance(feature_names=vars_x_onlystring, model = toponym_xb_onlystring)
xgb.plot.importance(importance_importance)

#importance_importance <- xgb.importance(feature_names=vars_x_onlystring, model = toponym_xb_onlystring_extra_large)
#xgb.plot.importance(importance_importance)

#importance_importance <- xgb.importance(feature_names=vars_x_everything, model = toponym_xb_everything2_extra_large)
#xgb.plot.importance(importance_importance)


#importance_importance <- xgb.importance(feature_names=vars_x_onlystring, model = toponym_xb_onlystring_uber_large)
#xgb.plot.importance(importance_importance)


```

# Analysis of Residuals

```{r}

global_test$toponym_xb_everything2_correct <- (global_test$toponym_xb_everything2>.5) == global_test$rex_match


#global_test$toponym_xb_onlystring_uber_large_correct <- (global_test$toponym_xb_onlystring_uber_large>.5)==global_test$rex_match
#table(global_test$toponym_xb_onlystring_uber_large_correct)


```


