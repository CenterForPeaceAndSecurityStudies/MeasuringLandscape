---
title: "Match Locations to Gazeteer and Show Differences"
author: "Rex W. Douglass"
date: "9/12/2016"
output:
  html_notebook:
    number_sections: yes
    theme: readable
    toc: yes
editor_options: 
  chunk_output_type: inline
---


```{r }
library(MeasuringLandscapeCivilWar)
devtools::load_all()


knitr::opts_knit$set(progress = TRUE, verbose = TRUE)
knitr::opts_chunk$set(fig.width=12, fig.height=8,  warning=FALSE, message=FALSE, cache=TRUE)
options(width = 160)

```
```{r}

#Load Events
events_sf <- readRDS(system.file("extdata", "events_sf.Rdata", package = "MeasuringLandscapeCivilWar")) 

events_sf_text_coord_unique <- ddply(events_sf[,c('location_text','name_clean','name_cleaner','document_district_clean','map_coordinate_clean_latitude','map_coordinate_clean_longitude')],
                                     .(location_text), transform,
      map_coordinate_has =sum(!is.na(map_coordinate_clean_latitude))
      )


```


```{r}

#Load toponym model
p_load(xgboost)
toponym_xb_model <- xgb.load(system.file("extdata", "toponym_xb_onlystring.bin", package = "MeasuringLandscapeCivilWar"))

```

```{r}

#Load Gazeteers
flatfiles_sf_roi <- readRDS(system.file("extdata", "flatfiles_sf_roi.Rdata", package = "MeasuringLandscapeCivilWar")) 
dim(flatfiles_sf_roi)
flatfiles_dt <- as.data.table(flatfiles_sf_roi)
setkey(flatfiles_dt,place_hash)

```


# Fuzzy Matcher Stage 1

```{r}

#Load Hand Labeled Examples
handlabeled <- fread(system.file("extdata",
                                 "event_flatfile_matches_for_hand_labeling - event_flatfile_matches_for_hand_labeling.csv",
                                 package = "MeasuringLandscapeCivilWar"), data.table=T) %>% distinct() 

#Remove exact matches because they're never interesting
handlabeled$stemmed_a <- strip_postfixes(handlabeled$name_cleaner_a)[[1]]
handlabeled$stemmed_b <- strip_postfixes(handlabeled$name_cleaner_b)[[1]]

handlabeled_unique <- subset(handlabeled, stemmed_a!=stemmed_b) # very important we're dropping any with identical stems for evaluation
table(handlabeled_unique$rex_match) #879 matches, 9455 nonmatches

#Stem them
handlabeled_unique$stemmed_a <- strip_postfixes(handlabeled_unique$name_cleaner_a)[[1]]
handlabeled_unique$stemmed_b <- strip_postfixes(handlabeled_unique$name_cleaner_b)[[1]]
handlabeled_unique$stemmed_ab <- sapply(lapply(strsplit(paste(handlabeled_unique$stemmed_a,
                                                              handlabeled_unique$stemmed_b, sep="_"),
                                                        "_"),
                                               sort),
                                        paste,
                                        collapse="_")

#handlabeled <- readRDS(file="/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/temp_everything.Rds")[[1]]
dim(handlabeled)
handlabeled$a <- handlabeled$name_cleaner_a
handlabeled$b <- handlabeled$name_cleaner_b
handlabeled[,ab:=paste(a,b,sep="_")]
handlabeled[,ba:=paste(b,a,sep="_")]

ab <- unique(c(handlabeled$stemmed_a, handlabeled$stemmed_b)) ; length(ab) #where ab is the unique toponym strings found in the data

#Generate qgrams using parameters we picked below
#ab_grams <- qgram_hash(strings=ab, n=1,k=0) ; dim(ab_grams)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=100, rows_per_band=4)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=100, rows_per_band=2)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=80, rows_per_band=2)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=40, rows_per_band=2)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=32, rows_per_band=16)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=64, rows_per_band=16)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=128, rows_per_band=16)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=256, rows_per_band=16)
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=512, rows_per_band=16) #1080 and 39 at 6.8 million , 3% error rate
# eval_lshr(strings=ab, grams=ab_grams, data=handlabeled_unique, bands_number=250, rows_per_band=8) #1087 and 3 at 16.8 million , 3% error rate

```



First let's calculate qgrams of various sizes and calculate pairwise correlation with the human labeled match/no match training set. A character ngram of size 4 has the highest pairwise spearman correlation of .84.

```{r}

nthread=detectCores()
pairwise_cor <- list()
for(i in 1:10){
  handlabeled[, q_gram := stringsim(a, b, "cosine", nthread = nthread, q = i), ]
  
  pairwise_cor[[i]] <- data.frame(q=i,
                                  correlation= cor(handlabeled$q_gram , handlabeled$rex_match, use="pairwise.complete.obs", method="spearman")
                                  )
}
pairwise_cor_df  <- rbindlist(pairwise_cor)

ggplot(pairwise_cor_df, aes(x=q,y=correlation)) + geom_point() + 
  ggtitle("Pairwise Correlation with Human Labeled Matches") + xlab("Qgram Size")


handlabeled[, q_gram_4 := stringsim(a, b, "cosine", nthread = nthread, q = 4), ]
handlabeled$q_gram_4[!is.finite(handlabeled$q_gram_4)] <- NA

ggplot(handlabeled, aes(y=q_gram_4, x=as.factor(rex_match))) + geom_boxplot() + 
  ggtitle("Pairwise Correlation with Human Labeled Matches") + xlab("No Match/Match")




#Find a conservative split
handlabeled
ab <- unique(c(handlabeled$a,
               handlabeled$b))
#First we hash each toponym, size 4, no skips
#3 grams is nearly as close but has a third of the columns
ab_grams <- qgram_hash(strings=ab,
                       n=3, #grams
                       k=0  #skips
                       ) ; dim(ab_grams) #5954, 5,146
rownames(ab_grams) <- ab    


p_load('proxy') # Library of similarity/dissimilarity measures for 'dist()'
handlabeled$q_gram_3_cosine_similarity <- 1 - 
            proxy::dist(x=as.matrix(ab_grams[handlabeled$a,]),
            y=as.matrix(ab_grams[handlabeled$b,]),
            method="cosine",
            pairwise=T, #
            by_rows=T #counter intuitive but right
            ) #Takes a short while but is pretty fast

#Cosine distance on a qgram is a little different than normalized string distance
ggplot(handlabeled, aes(y=q_gram_3_cosine_similarity, x=as.factor(rex_match))) + geom_boxplot() + 
  ggtitle("Pairwise Correlation with Human Labeled Matches") + xlab("No Match/Match") + ylab("Cosine Similarity")



#p_load(rpart)
p_load(party)
weights=rep(1, length(handlabeled$rex_match)) #Positive cases worth twice as much as negative cases
tree <- ctree(as.factor(rex_match) ~ q_gram_3_cosine_similarity,
              data=handlabeled,
              weights=weights,
              control=ctree_control(maxdepth=1)) #Find an optimal split 0.15
tree
plot(tree)
table(predict(tree),handlabeled$rex_match)

weights=handlabeled$rex_match+1 #Positive cases worth twice as much as negative cases
tree <- ctree(as.factor(rex_match) ~ q_gram_3_cosine_similarity,
              data=handlabeled,
              weights=weights,
              control=ctree_control(maxdepth=1)) #Find an optimal split 0.15
tree
plot(tree)
table(predict(tree),handlabeled$rex_match)

weights=(handlabeled$rex_match*9)+1 #Positive cases worth 10 times as much as negative cases
tree <- ctree(as.factor(rex_match) ~ q_gram_3_cosine_similarity,
              data=handlabeled,
              weights=weights,
              control=ctree_control(maxdepth=1)) #Find an optimal split 0.15
tree
plot(tree)
table(predict(tree),handlabeled$rex_match)
results_list <- list()
for(i in 0:100){
  print(i)
  weights=(handlabeled$rex_match*i)+1 #Positive cases worth 10 times as much as negative cases
  tree <- ctree(as.factor(rex_match) ~ q_gram_3_cosine_similarity,
                data=handlabeled,
                weights=weights,
                control=ctree_control(maxdepth=1)) #Find an optimal split 0.15
  #tree
  #plot(tree)
  d <- table(predict(tree),handlabeled$rex_match)
  results_list[[as.character(i)]] <- data.frame(i=i,
                                                true_negative=d[1,1],
                                                false_negative=d[1,2],
                                                false_positive=d[2,1],
                                                true_positive=d[2,2], 
                                                cosine_similarity=tree@tree$psplit$splitpoint)
}
results_df <- rbindlist(results_list)
results_df$true_negative  <- ( results_df$true_negative / sum(handlabeled$rex_match==0)  ) %>% round(digits=3)
results_df$false_negative <- ( results_df$false_negative / sum(handlabeled$rex_match==1) ) %>% round(digits=3)
results_df$false_positive     <- ( results_df$false_positive / sum(handlabeled$rex_match==0)     ) %>% round(digits=3)
results_df$true_positive      <- ( results_df$true_positive / sum(handlabeled$rex_match==1)      ) %>% round(digits=3)

results_df$cosine_similarity <- results_df$cosine_similarity %>% round(digits=2)
results_df_unique <- subset(results_df, !duplicated(cosine_similarity))
results_df_unique <- subset(results_df_unique, !duplicated(false_negative))

ggplot(results_df_unique,
       aes(false_negative,false_positive,
           label=cosine_similarity)) + 
      geom_text() + 
      ggtitle("Cosine Similarity Threshold and False Pos./Neg. Rate")



```

# Choosing a Locally Sensitive Hashing Threshold

We want to aproximate a cosine similarity of threshold of about 0.5. A good aproximation is 160 bands and 8 rows per pand, which intersects 50% chance of a positive at .5 similarity and near certainty at about .6 similarity.
 
```{r}

s_curve <- get_s_curve_new(
                       1000,
                       n_bands_min = 1,
                       n_rows_per_band_min = 1
                       )

s_curve <- get_s_curve_new(
                       32*10*4,
                       n_bands_min = 1,
                       n_rows_per_band_min = 1
                       )

```

```{r, eval=F, echo=F}

#Grid search I'm no longer doing

#This produces plots that help in choosing the hyper parameters used above
#They take a few minutes to run and output three plots
fromsrcatch=F
if(fromscratch){

    
    s_curve <- get_s_curve(
                           1280,
                           n_bands_min = 1,
                           n_rows_per_band_min = 1
                           )
    
    #Error: rows_per_band <= 32L is not TRUE
    s_curve <- subset(s_curve, n_rows_per_band<=32)
    
    #using cosine you can't have more than 32 bands, so starting at 80   
    s_curve$n_bands_n_rows_per_band <- paste(s_curve$n_bands,s_curve$n_rows_per_band, sep=":")
    s_curve_unique <- s_curve %>%
                       filter(!duplicated(n_bands_n_rows_per_band) &
                                n_bands>=100 #more bands means higher aproximate probability of matching if similarity is about some amount
                              )
    
    
    
    eval_list <- list()
    for(i in 1:nrow(s_curve_unique)) {
      print(i)
      eval_list[[i]] <-   eval_lshr(
                                    strings=ab,
                                    grams=ab_grams ,
                                    data=handlabeled_unique,
                                    bands_number=s_curve_unique$n_bands[i],
                                    rows_per_band=s_curve_unique$n_rows_per_band[i]
                                    )
      print(eval_list[[i]])
    }
    evaluations_varying_bands <- rbindlist(eval_list)
    evaluations_varying_bands$bands_number_rows_per_band <- paste(evaluations_varying_bands$bands_number,evaluations_varying_bands$rows_per_band, sep=":")
    evaluations_varying_bands$recall <- round( evaluations_varying_bands$misses/ (evaluations_varying_bands$hits + evaluations_varying_bands$misses ) , 2)
    evaluations_varying_bands$suggestions_per_fraction <- evaluations_varying_bands$suggestions_per/length(ab)
    
    saveRDS(evaluations_varying_bands,
        "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/evaluations_varying_bands.Rds") #.011 is about 300 and 0.015
}
    
    evaluations_varying_bands <- readRDS("/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscape/inst/extdata/evaluations_varying_bands.Rds") 

    p2 <- ggplot(evaluations_varying_bands,
         aes(x=recall,y=suggestions_per+1, label=bands_number_rows_per_band))  + geom_text(size=2) +
     #scale_y_continuous(breaks = round(seq(0, max(evaluations_varying_bands$suggestions_per), by = 100),1)) +
     scale_x_continuous(breaks = round(seq(0, max(evaluations_varying_bands$recall), by = .1),1)) + 
    xlab("1-Recall") + ylab("Number of Suggestions Per Item") + 
    ggtitle("Suggestion Count and Recall for LHS Parameters (Bands:Rows)") + theme_bw() +
       scale_y_log10()
    
    
```


     
```{r, eval=F, echo=F}

# GRID Search over different ngram features
  
fromsrcatch=F
if(fromscratch){
  
    #Then figure out what features we want
    eval_list <- list()
    for(i in 1:5){
      for(k in 0:5){
        index <- paste(i,k, sep="_")
        print(index)
        ab_grams <- qgram_hash(strings=ab, n=i,k=k) ; dim(ab_grams)
        eval_list[[index]] <- eval_lshr(strings=ab, grams=ab_grams , data=handlabeled_unique, bands_number=400, rows_per_band=5)
        eval_list[[index]]$ngrams=i
        eval_list[[index]]$skips=k
      }
    }
    evaluations_varying_ngrams <- rbindlist(eval_list)
    setkey(evaluations_varying_ngrams, ngrams, skips)
    #apparently ngrams1 of 1 with skips just tack on extra columns of the same thing
    evaluations_varying_ngrams$ngrams_skips <- paste(evaluations_varying_ngrams$ngrams,evaluations_varying_ngrams$skips, sep=":")
    evaluations_varying_ngrams$recall <- round( evaluations_varying_ngrams$misses/ (evaluations_varying_ngrams$hits + evaluations_varying_ngrams$misses ) , 2)

    saveRDS(evaluations_varying_ngrams,
            "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/evaluations_varying_ngrams.Rds") #.011 is about 300 and 0.015


}

    evaluations_varying_ngrams <- readRDS("/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/evaluations_varying_ngrams.Rds") 

    p1 <- ggplot(evaluations_varying_ngrams,
           aes(x=recall,y=suggestions_per, label=ngrams_skips))  + geom_text(size=2) +
       #scale_y_continuous(breaks = round(seq(0, max(evaluations_varying_ngrams$suggestions_per), by = 100),1)) +
       scale_x_continuous(breaks = round(seq(0, max(evaluations_varying_ngrams$recall), by = .05),2)) + 
      xlab("1-Recall") + ylab("Number of Suggestions Per Item") + 
      ggtitle("Suggestion Count and Recall for Character Gram Types (Grams:Skips)") + theme_bw() +
       scale_y_log10()
    p1

```
   
    
```{r, eval=F, echo=F}
    p_load(cowplot)
    p_combined <- plot_grid(p1, p2,
                            #labels = c("A", "B"),
                            align = "h")
    save_plot("/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/analysis/figures/suggester_grid_search.pdf", p_combined,
          base_aspect_ratio = 1.3 , # make room for figure legend
          base_width=10
    )


```



```{r, eval=F, echo=F}


    eval_lshr(bands_number=160, rows_per_band=8)
    
    p_load(parallelDist)
    d <- parDist(as.matrix(stemmed_ab_grams), method = "binary") #calculate the jaccard distance directly
    saveRDS(d,"/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/humanlabeled_ngram_dist_2_1_skips.Rds") #.011 is about 300 and 0.015

    #saveRDS(d,"/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/humanlabeled_ngram_dist_5.Rds") #.011 is about 300 and 0.015
    #saveRDS(d,"/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/humanlabeled_ngram_dist_6.Rds") #.011 is about 200 and 0.02
    
    d_m <- as.matrix(d)
    #make sure the row and col names are right
    #convert to
    hist(d_m, breaks=50)
    colnames(d_m) <- stemmed_ab
    rownames(d_m) <- stemmed_ab
    
    d_m_long = data.table::melt(d_m) ; dim(d_m_long)
    d_m_long$rex_match <- F
    d_m_long <- as.data.table(d_m_long)
    d_m_long[,ab:=paste(Var1, Var2, sep="_"),]
    d_m_long[,ba:=paste(Var2, Var1, sep="_"),]
    
    condition <- d_m_long$ab %in% subset(handlabeled, rex_match==1)$stemmed_ab | d_m_long$ba %in% subset(handlabeled, rex_match==1)$stemmed_ab ; table(condition)
    d_m_long$rex_match[condition] <- T
    table(d_m_long$rex_match)
    
    d_m_long <- subset(d_m_long, value!=0) #exclude any with 0 distance, those aren't interesting
    
    p_load(ggjoy)
    p <- ggplot(d_m_long, aes(x = value, y = rex_match)) + geom_joy2() 
    p + ggtitle("Pairwise Character Gram Profile Distance") + ylab("Hand Labeled Match") + xlab("Jaccard Distance")

```



