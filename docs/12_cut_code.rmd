---
title: "12 Cut Code"
author: "Rex W. Douglass and Kristen Harkness"
date: "March 9, 2018"
output: 
  html_notebook:
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: inline
---
<style>
    body .main-container {
        max-width: 100%;
    }
</style>


Chunks of code that were set to never run (eval=F) and were moved out of their relevant sections for clarity.

# from 03 Fuzzy Matcher

```{r 03 Fuzzy Matcher Cut 1, eval=F, echo=F}

#Grid search I'm no longer doing

#This produces plots that help in choosing the hyper parameters used above
#They take a few minutes to run and output three plots
fromscratch=T
if(fromscratch){

    
    s_curve <- get_s_curve_new(
                           1000,
                           n_bands_min = 1,
                           n_rows_per_band_min = 1
                           )
    
    #Error: rows_per_band <= 32L is not TRUE
    s_curve <- subset(s_curve, n_rows_per_band<=32)
    
    #using cosine you can't have more than 32 bands, so starting at 80   
    s_curve$n_bands_n_rows_per_band <- paste(s_curve$n_bands,s_curve$n_rows_per_band, sep=":")
    s_curve_unique <- s_curve %>%
                       filter(!duplicated(n_bands_n_rows_per_band) &
                                n_bands>=100 #more bands means higher aproximate probability of matching if similarity is about some amount
                              )
    
    s_curve_unique
    
    eval_list <- list()
    for(i in 1:nrow(s_curve_unique)) {
      print(i)
      eval_list[[i]] <-   eval_lshr(
                                    strings=stemmed_ab,
                                    grams=ab_grams_scaled ,
                                    data=handlabeled_unique,
                                    bands_number=s_curve_unique$n_bands[i],
                                    rows_per_band=s_curve_unique$n_rows_per_band[i]
                                    )
      print(eval_list[[i]])
    }
    evaluations_varying_bands <- rbindlist(eval_list)
    evaluations_varying_bands$bands_number_rows_per_band <- paste(evaluations_varying_bands$bands_number,evaluations_varying_bands$rows_per_band, sep=":")
    evaluations_varying_bands$recall <- round( evaluations_varying_bands$misses/ (evaluations_varying_bands$hits + evaluations_varying_bands$misses ) , 2)
    evaluations_varying_bands$suggestions_per_fraction <- evaluations_varying_bands$suggestions_per/length(ab)
    
    #saveRDS(evaluations_varying_bands,
    #    "/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscapeCivilWar/inst/extdata/evaluations_varying_bands.Rds") #.011 is about 300 and 0.015
}
    
    #evaluations_varying_bands <- readRDS("/home/rexdouglass/Dropbox (rex)/Kenya Article Drafts/MeasuringLandscape/inst/extdata/evaluations_varying_bands.Rds") 

    p2 <- ggplot(evaluations_varying_bands,
         aes(x=recall,y=suggestions_per+1, label=bands_number_rows_per_band))  + geom_text(size=2) +
     #scale_y_continuous(breaks = round(seq(0, max(evaluations_varying_bands$suggestions_per), by = 100),1)) +
     scale_x_continuous(breaks = round(seq(0, max(evaluations_varying_bands$recall), by = .1),1)) + 
    xlab("1-Recall") + ylab("Number of Suggestions Per Item") + 
    ggtitle("Suggestion Count and Recall for LHS Parameters (Bands:Rows)") + theme_bw() +
       scale_y_log10()
    p2
    
```




     
```{r 03 Fuzzy Matcher Cut 2, eval=F, echo=F}

# GRID Search over different ngram features
  
fromscratch=T
if(fromscratch){
  
    #Then figure out what features we want
    eval_list <- list()
    for(i in 1:5){
      for(k in 0:5){
        index <- paste(i,k, sep="_")
        print(index)
        ab_grams <- qgram_hash(strings=stemmed_ab, n=i,k=k) ; dim(ab_grams)
        eval_list[[index]] <- eval_lshr(strings=stemmed_ab, grams=ab_grams , data=handlabeled_unique, bands_number=400, rows_per_band=5)
        eval_list[[index]]$ngrams=i
        eval_list[[index]]$skips=k
      }
    }
    evaluations_varying_ngrams <- rbindlist(eval_list)
    setkey(evaluations_varying_ngrams, ngrams, skips)
    #apparently ngrams1 of 1 with skips just tack on extra columns of the same thing
    evaluations_varying_ngrams$ngrams_skips <- paste(evaluations_varying_ngrams$ngrams,evaluations_varying_ngrams$skips, sep=":")
    evaluations_varying_ngrams$recall <- round( evaluations_varying_ngrams$misses/ (evaluations_varying_ngrams$hits + evaluations_varying_ngrams$misses ) , 2)

    saveRDS(evaluations_varying_ngrams, paste0(here::here(), "/inst/extdata/evaluations_varying_ngrams.Rds")) #.011 is about 300 and 0.015


}

evaluations_varying_ngrams <- readRDS(paste0(here::here(), "/inst/extdata/evaluations_varying_ngrams.Rds")) 

p1 <- ggplot(evaluations_varying_ngrams,
       aes(x=recall,y=suggestions_per, label=ngrams_skips))  + geom_text(size=2) +
   #scale_y_continuous(breaks = round(seq(0, max(evaluations_varying_ngrams$suggestions_per), by = 100),1)) +
   scale_x_continuous(breaks = round(seq(0, max(evaluations_varying_ngrams$recall), by = .05),2)) + 
  xlab("1-Recall") + ylab("Number of Suggestions Per Item") + 
  ggtitle("Suggestion Count and Recall for Character Gram Types (Grams:Skips)") + theme_bw() +
   scale_y_log10()
p1

```



    
```{r 03 Fuzzy Matcher Cut 3, eval=F, echo=F}
    #p_load(cowplot)
    p_combined <- cowplot::plot_grid(p1, p2,
                            #labels = c("A", "B"),
                            align = "h")
    save_plot(paste0(here::here(), "/paper/figures/suggester_grid_search.pdf"), p_combined,
          base_aspect_ratio = 1.3 , # make room for figure legend
          base_width=10
    )


```



```{r 03 Fuzzy Matcher Cut 4, eval=F, echo=F}

eval_lshr(strings=stemmed_ab, grams=ab_grams, data=handlabeled_unique, bands_number=160, rows_per_band=8)

#p_load(parallelDist)
d <- parallelDist::parDist(as.matrix(stemmed_ab_grams), method = "binary") #calculate the jaccard distance directly
saveRDS(d, paste0(here::here(), "/inst/extdata/humanlabeled_ngram_dist_2_1_skips.Rds")) #.011 is about 300 and 0.015

d_m <- as.matrix(d)
#make sure the row and col names are right
#convert to
hist(d_m, breaks=50)
colnames(d_m) <- stemmed_ab
rownames(d_m) <- stemmed_ab

d_m_long = data.table::melt(d_m) ; dim(d_m_long)
d_m_long$rex_match <- F
d_m_long <- as.data.table(d_m_long)
d_m_long[,ab:=paste(Var1, Var2, sep="_"),]
d_m_long[,ba:=paste(Var2, Var1, sep="_"),]

condition <- d_m_long$ab %in% subset(handlabeled, rex_match==1)$stemmed_ab | d_m_long$ba %in% subset(handlabeled, rex_match==1)$stemmed_ab ; table(condition)
d_m_long$rex_match[condition] <- T
table(d_m_long$rex_match)

d_m_long <- subset(d_m_long, value!=0) #exclude any with 0 distance, those aren't interesting

#p_load(ggjoy)
p <- ggplot(d_m_long, aes(x = value, y = rex_match)) + geom_joy2() 
p + ggtitle("Pairwise Character Gram Profile Distance") + ylab("Hand Labeled Match") + xlab("Jaccard Distance")

```

# from 06 Ensemble and Hand Rules

```{r 06 Ensemble and Hand Rules Cut 1, eval=F}
vars_x_string <- c(
    "Jaro",
    "Optimal_String_Alignment"    ,
    "Levenshtein",
    "Damerau_Levenshtein"    ,
     "Longest_Common_Substring"     ,
    "q_gram_1",
    "q_gram_2",
    "q_gram_3",
    "q_gram_4",
    "q_gram_5",
    'Cosine_1',
    'Cosine_2',
    'Cosine_3',
    'Cosine_4',
    'Cosine_5',
    "Jaccard"              ,
     "First_Mistmatch"         ,
    "a_nchar"     ,
    "b_nchar"   ,
    "ab_nchar_diff"       ,
    "dJaro",
    "dOptimal_String_Alignment"      ,
    "dLevenshtein"     ,
    "dDamerau_Levenshtein"  ,
    "dLongest_Common_Substring",
    "dq_gram",
    "dCosine",
    "dJaccard"
)


georef_all_dt$a <- georef_all_dt$name_cleaner
georef_all_dt$b <- georef_all_dt$georef_b
georef_all_dt <- toponym_add_distances_dt(georef_all_dt)

dpredict<-xgb.DMatrix(data= as.matrix(georef_all_dt[,vars_x_string, with=F]), missing = NA)
georef_all_dt$toponym_xb_model_prediction <- predict( toponym_xb_model, dpredict )
georef_all_dt$toponym_xb_model_prediction  <- 1/(1 + exp(-georef_all_dt$toponym_xb_model_prediction ))

```


